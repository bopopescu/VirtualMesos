I0329 04:37:57.232223 31590 launcher.cpp:342] ExecutorLauncher::setupEnvironment
I0329 04:37:57.232324 31590 launcher.cpp:347] ExecutorLauncher::setupEnvironment: MESOS_DIRECTORY: /media/LinuxShare2/mesos/work/slaves/201203290434-0-1/frameworks/201203290434-0-0002/executors/default/runs/0
I0329 04:37:57.232650 31590 launcher.cpp:349] ExecutorLauncher::setupEnvironment: MESOS_SLAVE_PID: slave@192.168.1.64:53692
I0329 04:37:57.232734 31590 launcher.cpp:351] ExecutorLauncher::setupEnvironment: MESOS_FRAMEWORK_ID: 201203290434-0-0002
I0329 04:37:57.232846 31590 launcher.cpp:353] ExecutorLauncher::setupEnvironment: MESOS_EXECUTOR_ID: default
I0329 04:37:57.232961 31590 launcher.cpp:361] ExecutorLauncher::setupEnvironment: MESOS_HOME: /media/LinuxShare2/mesos
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/spark-distro/lib_managed/jars/org.slf4j/slf4j-log4j12/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/spark-distro/lib_managed/jars/hadoop_root/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
12/03/29 04:37:58 INFO spark.Executor: Running task ID 1
12/03/29 04:37:58 INFO spark.Executor: Running task ID 2
12/03/29 04:37:58 INFO spark.Executor: Running task ID 0
12/03/29 04:37:58 INFO spark.Executor: Running task ID 3
12/03/29 04:37:58 INFO spark.Executor: Running task ID 4
12/03/29 04:37:58 INFO spark.Executor: Running task ID 5
12/03/29 04:37:58 INFO spark.Executor: Running task ID 6
12/03/29 04:37:58 INFO spark.Executor: Running task ID 7
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 2
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 0
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 1
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 4
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 6
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 5
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 3
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 7
12/03/29 04:37:59 INFO spark.Executor: Running task ID 8
12/03/29 04:37:59 INFO spark.Executor: Running task ID 9
12/03/29 04:37:59 INFO spark.Executor: Running task ID 11
12/03/29 04:37:59 INFO spark.Executor: Running task ID 10
12/03/29 04:37:59 INFO spark.Executor: Running task ID 13
12/03/29 04:37:59 INFO spark.Executor: Running task ID 14
12/03/29 04:37:59 INFO spark.Executor: Running task ID 15
12/03/29 04:37:59 INFO spark.Executor: Running task ID 12
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 8
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 9
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 13
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 10
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 11
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 12
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 14
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 15
12/03/29 04:37:59 INFO spark.Executor: Running task ID 16
12/03/29 04:37:59 INFO spark.Executor: Running task ID 17
12/03/29 04:37:59 INFO spark.Executor: Running task ID 18
12/03/29 04:37:59 INFO spark.Executor: Running task ID 19
12/03/29 04:37:59 INFO spark.Executor: Running task ID 20
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 16
12/03/29 04:37:59 INFO spark.Executor: Running task ID 21
12/03/29 04:37:59 INFO spark.Executor: Running task ID 22
12/03/29 04:37:59 INFO spark.Executor: Running task ID 23
12/03/29 04:37:59 INFO spark.Executor: Running task ID 24
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 17
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 18
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 20
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 19
12/03/29 04:37:59 INFO spark.Executor: Running task ID 25
12/03/29 04:37:59 INFO spark.Executor: Running task ID 26
12/03/29 04:37:59 INFO spark.Executor: Running task ID 27
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 24
12/03/29 04:37:59 INFO spark.Executor: Running task ID 28
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 22
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 23
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 21
12/03/29 04:37:59 INFO spark.Executor: Running task ID 29
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 25
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 26
12/03/29 04:37:59 INFO spark.Executor: Running task ID 30
12/03/29 04:37:59 INFO spark.Executor: Running task ID 32
12/03/29 04:37:59 INFO spark.Executor: Running task ID 31
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 28
12/03/29 04:37:59 INFO spark.Executor: Running task ID 33
12/03/29 04:37:59 INFO spark.Executor: Running task ID 34
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 29
12/03/29 04:37:59 INFO spark.Executor: Running task ID 36
12/03/29 04:37:59 INFO spark.Executor: Running task ID 35
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 32
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 30
12/03/29 04:37:59 INFO spark.Executor: Running task ID 37
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 31
12/03/29 04:37:59 INFO spark.Executor: Running task ID 38
12/03/29 04:37:59 INFO spark.Executor: Finished task ID 33
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 34
12/03/29 04:38:00 INFO spark.Executor: Running task ID 39
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 36
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 35
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 37
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 38
12/03/29 04:38:00 INFO spark.Executor: Running task ID 40
12/03/29 04:38:00 INFO spark.Executor: Running task ID 41
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 27
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 39
12/03/29 04:38:00 INFO spark.Executor: Running task ID 42
12/03/29 04:38:00 INFO spark.Executor: Running task ID 43
12/03/29 04:38:00 INFO spark.Executor: Running task ID 44
12/03/29 04:38:00 INFO spark.Executor: Running task ID 45
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 40
12/03/29 04:38:00 INFO spark.Executor: Running task ID 46
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 41
12/03/29 04:38:00 INFO spark.Executor: Running task ID 47
12/03/29 04:38:00 INFO spark.Executor: Running task ID 49
12/03/29 04:38:00 INFO spark.Executor: Running task ID 48
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 44
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 42
12/03/29 04:38:00 INFO spark.Executor: Running task ID 50
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 43
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 45
12/03/29 04:38:00 INFO spark.Executor: Running task ID 51
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 46
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 49
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 47
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 48
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 50
12/03/29 04:38:00 INFO spark.Executor: Running task ID 52
12/03/29 04:38:00 INFO spark.Executor: Running task ID 53
12/03/29 04:38:00 INFO spark.Executor: Running task ID 54
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 51
12/03/29 04:38:00 INFO spark.Executor: Running task ID 55
12/03/29 04:38:00 INFO spark.Executor: Running task ID 56
12/03/29 04:38:00 INFO spark.Executor: Running task ID 57
12/03/29 04:38:00 INFO spark.Executor: Running task ID 58
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 52
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 54
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 53
12/03/29 04:38:00 INFO spark.Executor: Running task ID 59
12/03/29 04:38:00 INFO spark.Executor: Running task ID 60
12/03/29 04:38:00 INFO spark.Executor: Running task ID 61
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 56
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 57
12/03/29 04:38:00 INFO spark.Executor: Running task ID 63
12/03/29 04:38:00 INFO spark.Executor: Running task ID 62
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 58
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 55
12/03/29 04:38:00 INFO spark.Executor: Running task ID 64
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 59
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 60
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 61
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 63
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 62
12/03/29 04:38:00 INFO spark.Executor: Finished task ID 64
12/03/29 04:38:05 INFO spark.Executor: Running task ID 65
12/03/29 04:38:05 INFO spark.Executor: Running task ID 67
12/03/29 04:38:05 INFO spark.Executor: Running task ID 69
12/03/29 04:38:05 INFO spark.Executor: Running task ID 71
12/03/29 04:38:05 INFO spark.Executor: Running task ID 73
12/03/29 04:38:05 INFO spark.Executor: Running task ID 75
12/03/29 04:38:05 INFO spark.Executor: Running task ID 79
12/03/29 04:38:05 INFO spark.Executor: Running task ID 77
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 65
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 73
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 69
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 79
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 77
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 67
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 75
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 71
12/03/29 04:38:05 INFO spark.Executor: Running task ID 81
12/03/29 04:38:05 INFO spark.Executor: Running task ID 82
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 81
12/03/29 04:38:05 INFO spark.Executor: Running task ID 83
12/03/29 04:38:05 INFO spark.Executor: Running task ID 84
12/03/29 04:38:05 INFO spark.Executor: Running task ID 85
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 82
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 84
12/03/29 04:38:05 INFO spark.Executor: Finished task ID 83
12/03/29 04:38:05 INFO spark.Executor: Running task ID 86
12/03/29 04:38:05 INFO spark.Executor: Running task ID 87
12/03/29 04:38:05 INFO spark.Executor: Running task ID 88
12/03/29 04:38:06 INFO spark.Executor: Running task ID 89
12/03/29 04:38:06 INFO spark.Executor: Running task ID 90
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 85
12/03/29 04:38:06 INFO spark.Executor: Running task ID 91
12/03/29 04:38:06 INFO spark.Executor: Running task ID 92
12/03/29 04:38:06 INFO spark.Executor: Running task ID 93
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 88
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 87
12/03/29 04:38:06 INFO spark.Executor: Running task ID 94
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 89
12/03/29 04:38:06 INFO spark.Executor: Running task ID 96
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 86
12/03/29 04:38:06 INFO spark.Executor: Running task ID 95
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 90
12/03/29 04:38:06 INFO spark.Executor: Running task ID 97
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 92
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 93
12/03/29 04:38:06 INFO spark.Executor: Running task ID 98
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 91
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 95
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 94
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 96
12/03/29 04:38:06 INFO spark.Executor: Running task ID 99
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 97
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 98
12/03/29 04:38:06 INFO spark.Executor: Finished task ID 99
12/03/29 04:38:08 INFO spark.Executor: Running task ID 100
12/03/29 04:38:08 INFO spark.Executor: Running task ID 101
12/03/29 04:38:08 INFO spark.Executor: Running task ID 102
12/03/29 04:38:08 INFO spark.Executor: Running task ID 104
12/03/29 04:38:08 INFO spark.Executor: Running task ID 103
12/03/29 04:38:08 INFO spark.Executor: Running task ID 107
12/03/29 04:38:08 INFO spark.Executor: Running task ID 106
12/03/29 04:38:08 INFO spark.Executor: Running task ID 105
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 104
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 103
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 106
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 100
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 101
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 107
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 102
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 105
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 INFO spark.Executor: Running task ID 108
12/03/29 04:38:08 ERROR spark.Executor: Exception in task ID 108
java.lang.ClassCastException: cannot assign instance of hadooproot.HadoopRootRDD$$anonfun$2 to field spark.Aggregator.mergeCombiners of type scala.Function2 in instance of spark.Aggregator
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2056)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1229)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1969)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 04:38:08 INFO spark.Executor: Running task ID 109
12/03/29 04:38:08 INFO spark.Executor: Running task ID 110
12/03/29 04:38:08 INFO spark.Executor: Running task ID 111
