I0329 11:26:33.601464 28593 launcher.cpp:342] ExecutorLauncher::setupEnvironment
I0329 11:26:33.601567 28593 launcher.cpp:347] ExecutorLauncher::setupEnvironment: MESOS_DIRECTORY: /media/LinuxShare2/mesos/work/slaves/201203291014-0-4/frameworks/201203291014-0-0018/executors/default/runs/2
I0329 11:26:33.601748 28593 launcher.cpp:349] ExecutorLauncher::setupEnvironment: MESOS_SLAVE_PID: slave@192.168.1.64:40765
I0329 11:26:33.601842 28593 launcher.cpp:351] ExecutorLauncher::setupEnvironment: MESOS_FRAMEWORK_ID: 201203291014-0-0018
I0329 11:26:33.601963 28593 launcher.cpp:353] ExecutorLauncher::setupEnvironment: MESOS_EXECUTOR_ID: default
I0329 11:26:33.602090 28593 launcher.cpp:361] ExecutorLauncher::setupEnvironment: MESOS_HOME: /media/LinuxShare2/mesos
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/spark-distro/lib_managed/jars/org.slf4j/slf4j-log4j12/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/spark-distro/lib_managed/jars/hadoop_root/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
12/03/29 11:26:34 INFO spark.Executor: Running task ID 34
12/03/29 11:26:34 INFO spark.Executor: Running task ID 31
12/03/29 11:26:34 INFO spark.Executor: Running task ID 30
12/03/29 11:26:34 INFO spark.Executor: Running task ID 35
12/03/29 11:26:34 INFO spark.Executor: Running task ID 36
12/03/29 11:26:34 INFO spark.Executor: Running task ID 32
12/03/29 11:26:34 INFO spark.Executor: Running task ID 33
12/03/29 11:26:34 INFO spark.Executor: Running task ID 29
12/03/29 11:26:34 INFO spark.CacheTracker: CachedRDD partition key is (1,3)
12/03/29 11:26:34 INFO spark.CacheTracker: CachedRDD partition key is (1,0)
12/03/29 11:26:34 INFO spark.CacheTracker: CachedRDD partition key is (1,4)
12/03/29 11:26:34 INFO spark.CacheTracker: CachedRDD partition key is (1,7)
12/03/29 11:26:34 INFO spark.CacheTracker: CachedRDD partition key is (1,2)
12/03/29 11:26:34 INFO spark.CacheTracker: CachedRDD partition key is (1,1)
12/03/29 11:26:34 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@692
12/03/29 11:26:34 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@693
12/03/29 11:26:34 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@694
12/03/29 11:26:34 INFO spark.CacheTracker: CachedRDD partition key is (1,6)
12/03/29 11:26:34 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@697
12/03/29 11:26:34 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@698
12/03/29 11:26:34 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@691
12/03/29 11:26:34 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@695
12/03/29 11:26:34 INFO spark.CacheTracker: CachedRDD partition key is (1,5)
12/03/29 11:26:34 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@696
12/03/29 11:26:35 ERROR spark.Executor: Exception in task ID 29
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1362)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1170)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.Utils$.serialize(Utils.scala:18)
	at spark.Executor$TaskRunner.run(Executor.scala:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 11:26:35 ERROR spark.Executor: Exception in task ID 33
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1362)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1170)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.Utils$.serialize(Utils.scala:18)
	at spark.Executor$TaskRunner.run(Executor.scala:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 11:26:35 ERROR spark.Executor: Exception in task ID 31
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1362)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1170)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.Utils$.serialize(Utils.scala:18)
	at spark.Executor$TaskRunner.run(Executor.scala:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 11:26:35 ERROR spark.Executor: Exception in task ID 32
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1362)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1170)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.Utils$.serialize(Utils.scala:18)
	at spark.Executor$TaskRunner.run(Executor.scala:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 11:26:35 ERROR spark.Executor: Exception in task ID 36
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1362)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1170)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.Utils$.serialize(Utils.scala:18)
	at spark.Executor$TaskRunner.run(Executor.scala:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 11:26:35 ERROR spark.Executor: Exception in task ID 34
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1362)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1170)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.Utils$.serialize(Utils.scala:18)
	at spark.Executor$TaskRunner.run(Executor.scala:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:240)
	at org.apache.hadoop.hdfs.DFSClient.access$800(DFSClient.java:73)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2042)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:153)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:194)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:99)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:250)
	at scala.collection.Iterator$$anon$19.toBuffer(Iterator.scala:333)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:237)
	at scala.collection.Iterator$$anon$19.toArray(Iterator.scala:333)
	at spark.CacheTracker.getOrCompute(CacheTracker.scala:134)
	at spark.RDD.iterator(RDD.scala:76)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 11:26:35 ERROR spark.Executor: Exception in task ID 35
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1362)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1170)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.Utils$.serialize(Utils.scala:18)
	at spark.Executor$TaskRunner.run(Executor.scala:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 11:26:35 ERROR spark.Executor: Exception in task ID 30
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1362)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1170)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.Utils$.serialize(Utils.scala:18)
	at spark.Executor$TaskRunner.run(Executor.scala:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
