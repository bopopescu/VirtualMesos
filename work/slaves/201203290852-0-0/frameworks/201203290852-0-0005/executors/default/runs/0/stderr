I0329 09:04:39.908880 18206 launcher.cpp:342] ExecutorLauncher::setupEnvironment
I0329 09:04:39.909011 18206 launcher.cpp:347] ExecutorLauncher::setupEnvironment: MESOS_DIRECTORY: /media/LinuxShare2/mesos/work/slaves/201203290852-0-0/frameworks/201203290852-0-0005/executors/default/runs/0
I0329 09:04:39.909282 18206 launcher.cpp:349] ExecutorLauncher::setupEnvironment: MESOS_SLAVE_PID: slave@192.168.1.64:35806
I0329 09:04:39.909509 18206 launcher.cpp:351] ExecutorLauncher::setupEnvironment: MESOS_FRAMEWORK_ID: 201203290852-0-0005
I0329 09:04:39.910682 18206 launcher.cpp:353] ExecutorLauncher::setupEnvironment: MESOS_EXECUTOR_ID: default
I0329 09:04:39.910915 18206 launcher.cpp:361] ExecutorLauncher::setupEnvironment: MESOS_HOME: /media/LinuxShare2/mesos
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/spark-distro/lib_managed/jars/org.slf4j/slf4j-log4j12/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/spark-distro/lib_managed/jars/hadoop_root/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
12/03/29 09:04:40 INFO spark.Executor: Running task ID 0
12/03/29 09:04:40 INFO spark.Executor: Running task ID 4
12/03/29 09:04:40 INFO spark.Executor: Running task ID 3
12/03/29 09:04:40 INFO spark.Executor: Running task ID 1
12/03/29 09:04:40 INFO spark.Executor: Running task ID 2
12/03/29 09:04:40 INFO spark.Executor: Running task ID 5
12/03/29 09:04:40 INFO spark.Executor: Running task ID 6
12/03/29 09:04:40 INFO spark.Executor: Running task ID 7
12/03/29 09:04:41 INFO spark.CacheTracker: CachedRDD partition key is (1,3)
12/03/29 09:04:41 INFO spark.CacheTracker: CachedRDD partition key is (1,0)
12/03/29 09:04:41 INFO spark.CacheTracker: CachedRDD partition key is (1,6)
12/03/29 09:04:41 INFO spark.CacheTracker: CachedRDD partition key is (1,2)
12/03/29 09:04:41 INFO spark.CacheTracker: CachedRDD partition key is (1,5)
12/03/29 09:04:41 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@697
12/03/29 09:04:41 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@693
12/03/29 09:04:41 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@694
12/03/29 09:04:41 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@691
12/03/29 09:04:41 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@696
12/03/29 09:04:41 INFO spark.CacheTracker: CachedRDD partition key is (1,7)
12/03/29 09:04:41 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@698
12/03/29 09:04:41 INFO spark.CacheTracker: CachedRDD partition key is (1,1)
12/03/29 09:04:41 INFO spark.CacheTracker: CachedRDD partition key is (1,4)
12/03/29 09:04:41 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@695
12/03/29 09:04:41 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@692
12/03/29 09:04:42 INFO spark.Executor: Finished task ID 4
12/03/29 09:04:42 INFO spark.Executor: Finished task ID 1
12/03/29 09:04:42 INFO spark.Executor: Finished task ID 0
12/03/29 09:04:42 INFO spark.Executor: Finished task ID 2
12/03/29 09:04:42 INFO spark.Executor: Finished task ID 7
12/03/29 09:04:42 INFO spark.Executor: Finished task ID 3
12/03/29 09:04:42 INFO spark.Executor: Finished task ID 6
12/03/29 09:04:42 INFO spark.Executor: Finished task ID 5
12/03/29 09:04:42 INFO spark.Executor: Running task ID 8
12/03/29 09:04:42 INFO spark.Executor: Running task ID 9
12/03/29 09:04:42 INFO spark.Executor: Running task ID 10
12/03/29 09:04:42 INFO spark.Executor: Running task ID 11
12/03/29 09:04:42 INFO spark.Executor: Running task ID 12
12/03/29 09:04:42 INFO spark.Executor: Running task ID 14
12/03/29 09:04:42 INFO spark.Executor: Running task ID 13
12/03/29 09:04:42 INFO spark.Executor: Running task ID 15
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,8)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@699
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,15)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a0
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,10)
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,9)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@69b
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@69a
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,11)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@69c
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,12)
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,13)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@69e
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@69d
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,14)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@69f
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 8
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 10
12/03/29 09:04:43 INFO spark.Executor: Running task ID 16
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 11
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 13
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 9
12/03/29 09:04:43 INFO spark.Executor: Running task ID 17
12/03/29 09:04:43 INFO spark.Executor: Running task ID 18
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 12
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 14
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,16)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a1
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,18)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a3
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,17)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a2
12/03/29 09:04:43 INFO spark.Executor: Running task ID 19
12/03/29 09:04:43 INFO spark.Executor: Running task ID 20
12/03/29 09:04:43 INFO spark.Executor: Running task ID 21
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 18
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 17
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 16
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 15
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,19)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a4
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,20)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a5
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,21)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a6
12/03/29 09:04:43 INFO spark.Executor: Running task ID 22
12/03/29 09:04:43 INFO spark.Executor: Running task ID 23
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 19
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 20
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 21
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,22)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a7
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,23)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a8
12/03/29 09:04:43 INFO spark.Executor: Running task ID 24
12/03/29 09:04:43 INFO spark.Executor: Running task ID 25
12/03/29 09:04:43 INFO spark.Executor: Running task ID 26
12/03/29 09:04:43 INFO spark.Executor: Running task ID 27
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,24)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6a9
12/03/29 09:04:43 INFO spark.Executor: Running task ID 28
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 23
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 22
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,25)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6aa
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,27)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6ac
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,26)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6ab
12/03/29 09:04:43 INFO spark.Executor: Running task ID 29
12/03/29 09:04:43 INFO spark.Executor: Running task ID 30
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 25
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 24
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 26
12/03/29 09:04:43 INFO spark.Executor: Running task ID 31
12/03/29 09:04:43 INFO spark.Executor: Running task ID 32
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 27
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,30)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6af
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,28)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6ad
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,29)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6ae
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,32)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b1
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,31)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b0
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 30
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 28
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 29
12/03/29 09:04:43 INFO spark.Executor: Running task ID 33
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 32
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 31
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,33)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b2
12/03/29 09:04:43 INFO spark.Executor: Running task ID 34
12/03/29 09:04:43 INFO spark.Executor: Running task ID 35
12/03/29 09:04:43 INFO spark.Executor: Running task ID 36
12/03/29 09:04:43 INFO spark.Executor: Running task ID 37
12/03/29 09:04:43 INFO spark.Executor: Running task ID 38
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,34)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b3
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 33
12/03/29 09:04:43 INFO spark.Executor: Running task ID 39
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,37)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b6
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,35)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b4
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,36)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b5
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,38)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b7
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 34
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 37
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 35
12/03/29 09:04:43 INFO spark.CacheTracker: CachedRDD partition key is (1,39)
12/03/29 09:04:43 INFO spark.CacheTracker: Computing partition spark.HadoopSplit@6b8
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 36
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 38
12/03/29 09:04:43 INFO spark.Executor: Finished task ID 39
12/03/29 09:04:48 INFO spark.Executor: Running task ID 40
12/03/29 09:04:48 INFO spark.Executor: Running task ID 41
12/03/29 09:04:48 INFO spark.Executor: Running task ID 42
12/03/29 09:04:48 INFO spark.Executor: Running task ID 43
12/03/29 09:04:49 INFO spark.Executor: Running task ID 44
12/03/29 09:04:49 INFO spark.Executor: Running task ID 45
12/03/29 09:04:49 INFO spark.Executor: Running task ID 46
12/03/29 09:04:49 INFO spark.Executor: Running task ID 47
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,0)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.LocalFileShuffle: Shuffle dir: /tmp/spark-local-2916acb7-0960-457d-8764-175ac39288b7/shuffle
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,3)
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,5)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,7)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,4)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,6)
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,2)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,1)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO server.Server: jetty-7.5.3.v20111011
12/03/29 09:04:49 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:35835 STARTING
12/03/29 09:04:49 INFO spark.LocalFileShuffle: Local URI: http://192.168.1.64:35835
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 45
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 47
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 43
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 40
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 46
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 44
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 42
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 41
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 INFO spark.Executor: Running task ID 48
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,5)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 48
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 INFO spark.Executor: Running task ID 49
12/03/29 09:04:49 INFO spark.Executor: Running task ID 50
12/03/29 09:04:49 INFO spark.Executor: Running task ID 51
12/03/29 09:04:49 INFO spark.Executor: Running task ID 52
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,4)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,6)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,7)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 50
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 49
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 ERROR spark.Executor: Exception in task ID 52
java.io.NotSerializableException: org.apache.hadoop.io.Text
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
	at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:7)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at spark.ShuffleMapTask$$anonfun$run$1$$anonfun$apply$mcVI$sp$1.apply(ShuffleMapTask.scala:31)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:43)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:93)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:28)
	at spark.ShuffleMapTask.run(ShuffleMapTask.scala:9)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/29 09:04:49 INFO spark.CacheTracker: CachedRDD partition key is (1,3)
12/03/29 09:04:49 INFO spark.CacheTracker: Found partition in cache!
