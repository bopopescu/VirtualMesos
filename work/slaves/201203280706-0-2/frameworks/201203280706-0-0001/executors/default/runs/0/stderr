I0328 07:18:04.729244 32661 launcher.cpp:341] ExecutorLauncher::setupEnvironment
I0328 07:18:04.729295 32661 launcher.cpp:346] ExecutorLauncher::setupEnvironment: MESOS_DIRECTORY: /media/LinuxShare2/mesos/work/slaves/201203280706-0-2/frameworks/201203280706-0-0001/executors/default/runs/0
I0328 07:18:04.729325 32661 launcher.cpp:348] ExecutorLauncher::setupEnvironment: MESOS_SLAVE_PID: slave@192.168.1.64:35543
I0328 07:18:04.729339 32661 launcher.cpp:350] ExecutorLauncher::setupEnvironment: MESOS_FRAMEWORK_ID: 201203280706-0-0001
I0328 07:18:04.729352 32661 launcher.cpp:352] ExecutorLauncher::setupEnvironment: MESOS_EXECUTOR_ID: default
I0328 07:18:04.729368 32661 launcher.cpp:360] ExecutorLauncher::setupEnvironment: MESOS_HOME: /media/LinuxShare2/mesos
12/03/28 07:18:05 INFO spark.Executor: Running task ID 0
12/03/28 07:18:05 INFO spark.Executor: Running task ID 3
12/03/28 07:18:05 INFO spark.Executor: Running task ID 2
12/03/28 07:18:05 INFO spark.Executor: Running task ID 1
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:219)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:217)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:219)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:217)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:219)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:217)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:05 INFO spark.Executor: Finished task ID 0
12/03/28 07:18:05 INFO spark.Executor: Finished task ID 3
12/03/28 07:18:05 INFO spark.Executor: Finished task ID 1
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:219)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:217)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:06 INFO spark.Executor: Running task ID 4
12/03/28 07:18:06 INFO spark.Executor: Finished task ID 2
12/03/28 07:18:06 INFO spark.Executor: Running task ID 5
12/03/28 07:18:06 INFO spark.Executor: Running task ID 6
12/03/28 07:18:06 INFO spark.Executor: Running task ID 7
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:219)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:217)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:06 INFO spark.Executor: Finished task ID 4
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:219)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:217)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:06 INFO spark.Executor: Finished task ID 7
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:219)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:217)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:06 INFO spark.Executor: Finished task ID 5
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:219)
	at spark.RDD$$anonfun$count$1.apply(RDD.scala:217)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:266)
	at spark.ResultTask.run(ResultTask.scala:10)
	at spark.Executor$TaskRunner.run(Executor.scala:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:06 INFO spark.Executor: Finished task ID 6
12/03/28 07:18:29 INFO spark.Executor: Running task ID 8
12/03/28 07:18:29 INFO spark.Executor: Running task ID 9
12/03/28 07:18:29 INFO spark.Executor: Running task ID 10
12/03/28 07:18:29 INFO spark.Executor: Running task ID 11
/hadoop-distro/StdInReaderForArray: error while loading shared libraries: libTree.so: cannot open shared object file: No such file or directory
12/03/28 07:18:29 INFO spark.Executor: Finished task ID 8
/hadoop-distro/StdInReaderForArray: error while loading shared libraries: libTree.so: cannot open shared object file: No such file or directory
12/03/28 07:18:29 INFO spark.Executor: Finished task ID 10
12/03/28 07:18:29 INFO spark.Executor: Running task ID 12
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at spark.PipedRDD$$anon$2.run(PipedRDD.scala:41)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:29 INFO spark.Executor: Finished task ID 11
/hadoop-distro/StdInReaderForArray: error while loading shared libraries: libTree.so: cannot open shared object file: No such file or directory
/hadoop-distro/StdInReaderForArray: error while loading shared libraries: libTree.so: cannot open shared object file: No such file or directory
12/03/28 07:18:29 INFO spark.Executor: Finished task ID 9
12/03/28 07:18:29 INFO spark.Executor: Running task ID 13
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at spark.PipedRDD$$anon$2.run(PipedRDD.scala:41)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:29 INFO spark.Executor: Running task ID 14
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at spark.PipedRDD$$anon$2.run(PipedRDD.scala:41)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:29 INFO spark.Executor: Running task ID 15
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at spark.PipedRDD$$anon$2.run(PipedRDD.scala:41)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
/hadoop-distro/StdInReaderForArray: error while loading shared libraries: libTree.so: cannot open shared object file: No such file or directory
12/03/28 07:18:29 INFO spark.Executor: Finished task ID 12
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at spark.PipedRDD$$anon$2.run(PipedRDD.scala:41)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
/hadoop-distro/StdInReaderForArray: error while loading shared libraries: libTree.so: cannot open shared object file: No such file or directory
12/03/28 07:18:29 INFO spark.Executor: Finished task ID 14
/hadoop-distro/StdInReaderForArray: error while loading shared libraries: libTree.so: cannot open shared object file: No such file or directory
12/03/28 07:18:29 INFO spark.Executor: Finished task ID 13
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at spark.PipedRDD$$anon$2.run(PipedRDD.scala:41)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at spark.PipedRDD$$anon$2.run(PipedRDD.scala:41)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
/hadoop-distro/StdInReaderForArray: error while loading shared libraries: libTree.so: cannot open shared object file: No such file or directory
12/03/28 07:18:29 INFO spark.Executor: Finished task ID 15
java.io.IOException: Call to azad/192.168.1.64:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:775)
	at org.apache.hadoop.ipc.Client.call(Client.java:743)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:182)
	at com.virkaz.star.hadoopio.PipesBinaryMatrixRecordReader.next(PipesBinaryMatrixRecordReader.java:27)
	at spark.HadoopRDD$$anon$1.hasNext(HadoopRDD.scala:81)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:334)
	at scala.collection.Iterator$class.foreach(Iterator.scala:660)
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:333)
	at spark.PipedRDD$$anon$2.run(PipedRDD.scala:41)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:501)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)
12/03/28 07:18:29 INFO spark.Executor: Running task ID 16
12/03/28 07:18:29 INFO spark.Executor: Running task ID 17
12/03/28 07:18:29 INFO spark.Executor: Running task ID 18
12/03/28 07:18:29 INFO spark.Executor: Running task ID 19
12/03/28 07:18:29 ERROR spark.Executor: Exception in task ID 19
java.lang.ClassNotFoundException: hadooproot.HadoopRootRDD$$anonfun$main$1
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$findClass(ScalaClassLoader.scala:88)
	at scala.tools.nsc.util.ScalaClassLoader$class.findClass(ScalaClassLoader.scala:44)
	at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.findClass(ScalaClassLoader.scala:88)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$loadClass(ScalaClassLoader.scala:88)
	at scala.tools.nsc.util.ScalaClassLoader$class.loadClass(ScalaClassLoader.scala:50)
	at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.loadClass(ScalaClassLoader.scala:88)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at spark.Utils$$anon$1.resolveClass(Utils.scala:33)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1592)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1513)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1749)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
12/03/28 07:18:29 ERROR spark.Executor: Exception in task ID 16
java.lang.ClassNotFoundException: hadooproot.HadoopRootRDD$$anonfun$main$1
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$findClass(ScalaClassLoader.scala:88)
	at scala.tools.nsc.util.ScalaClassLoader$class.findClass(ScalaClassLoader.scala:44)
	at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.findClass(ScalaClassLoader.scala:88)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$loadClass(ScalaClassLoader.scala:88)
	at scala.tools.nsc.util.ScalaClassLoader$class.loadClass(ScalaClassLoader.scala:50)
	at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.loadClass(ScalaClassLoader.scala:88)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at spark.Utils$$anon$1.resolveClass(Utils.scala:33)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1592)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1513)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1749)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1963)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1887)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1770)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1346)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:368)
	at spark.Utils$.deserialize(Utils.scala:35)
	at spark.Executor$TaskRunner.run(Executor.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
